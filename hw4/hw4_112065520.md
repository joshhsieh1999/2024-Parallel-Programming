# PP HW 4 Report

## 1. Implementation

### 1.1 Implement FlashAttention Forward Pass Using CUDA

#### (a) Matrix Blocking

The input matrices `Q`, `K`, and `V` have dimensions `N x d`. To efficiently process them on the GPU, I divide them into smaller tiles:
   - `Q_i`: a tile of size `br x d` (e.g., `br = 128`).
   - `K_j`, `V_j`: tiles of size `bc x d` (e.g., `bc = 128`).

By choosing suitable tile dimensions (`br` and `bc`), the computations fit well into thread blocks and memory can be accessed in a coalesced fashion. For each `(i, j)` tile pair, I compute an attention submatrix `S_ij` of size `br x bc`. I set `br` and `bc` to 128 because according to the spec, `N` is at least 128. If `N` is less than 128, more tiles might be generated and the `flash_attention()` will have to run more iterations and more kernel launches, resulting in more overhead and excessive execution time. If `N` is larger than 128, out-of-bounds memory access may occur if not handled properly.

   The tiles looks like this:
   
   **Q_i Tile (br x d):**
   ```css
       d →  
    Q_i ┌─────────────────────────┐
        │ q_11  q_12  q_13 ... q_1d│
     b  │ q_21  q_22  q_23 ... q_2d│
     r  │ q_31  q_32  q_33 ... q_3d│
     ↓  │ ...                     ... 
        │ q_br1 q_br2 ...      q_brd│
        └─────────────────────────┘
   ```
   `br` is the number of rows in the tile, and `d` is the embedding dimension.
   
   **K_j Tile (bc x d):**
   ```css
       d →  
    K_j ┌─────────────────────────┐
        │ k_11  k_12  k_13 ... k_1d│
     b  │ k_21  k_22  k_23 ... k_2d│
     c  │ k_31  k_32  k_33 ... k_3d│
     ↓  │ ...                     ... 
        │ k_bc1 k_bc2 ...     k_bcd│
        └─────────────────────────┘
   ```
   `bc` is the number of columns(After transposing) in the tile, and `d` is the embedding dimension.
   
   **S_ij Tile (br x bc)**
   ```css
             bc →
    S_ij ┌────────────────────────────────┐
         │ s_11     s_12     s_13  ... s_1bc │
     b   │ s_21     s_22     s_23  ... s_2bc │
     r   │ s_31     s_32     s_33  ... s_3bc │
     ↓   │ ...                             ... 
         │ s_br1    s_br2    ...       s_brbc│
         └────────────────────────────────┘
   ```
   Each element `s_brbc` in `S_ij` is computed by taking the dot product of the r-th row of `Q_i` with the c-th row of `K_j`.

The `flash_attention()` will execute double `for` loops for a single batch. The outer `for` loop iterates through the columns, and the inner `for` loop iterates through all rows within the tile. Each column performs dot product on each row before moving on to the next column.

The outer loop `for (int j = 0; j < tc; j++)` iterates over the column tiles. Each iteration `j` represents a block of `bc` columns in the `N x N` matrix (except possibly the last one which might be smaller if `N` is not a multiple of `bc`).

The inner loop `for (int i = 0; i < tr; i++)` iterates over the row tiles. Each iteration `i` represents a block of `br` rows.

```c=
int br = 128, bc = 128;
int tr = (N + br - 1) / br; // round up
int tc = (N + bc - 1) / bc;
for (int j = 0; j < tc; j {
    for (int i = 0; i < tr; i++) {
        ...
        // Calculate dot product of every row with the column
        FusedQKDotAndRowOpsKernel<<<gridFused, blockFused>>>();
        ...
    }
}
```

The kernel launch parameters look like this:

```c=
dim3 blockFused(bc, 1);
dim3 gridFused(1, br);
size_t sharedMemSize = bc * sizeof(float);
```

I launched `br` blocks, where each block processes exactly one row within the `br x bc` tile. Within each block, `bc` threads are launched, and each thread handles one column element within the `br x bc` tile (handling an entire column in `bc` -> `d` elements).

#### (b) QK Dot Product and Scaling

Compute: $S_{ij} = Q_i K_j^T / \sqrt{d}$
For each `(i, j)` tile, a CUDA kernel loads the respective `Q_i` and `K_j` chunks and performs the dot product to form `S_ij`. Each thread is responsible for one element of `S_ij`. The scaling factor $1 / \sqrt{d}$ is applied to maintain numerical stability. Here I passed it as `scalar` variable into the kernel to prevent recalculation.

#### (c\) Row-Wise Max and Exponentiation:

After computing `S_ij`, find the maximum value in each row. This maximum is used to shift the values before exponentiation (`exp(S_ij - max)`) to prevent overflow. The process is:
   - Use warp-level and block-level reductions (implemented via CUDA built-ins such as `__shfl_xor_sync` or shared memory reductions) to find the row-wise maximum `m_ij`.
   - Compute the exponentials of the shifted values, `P_ij = exp(S_ij - m_ij)`.

**Code**:

```c=
// --- Warp-Level Reductions for row_max ---
    __shared__ float warp_max[4];

    unsigned mask = 0xffffffff;
    int laneId = col & 31;     
    int warpId = col >> 5;     

    // Find row_max via warp reduction
    float val = sum;
    #pragma unroll
    for (int offset = 16; offset > 0; offset >>= 1) {
        // __shfl_xor_sync(mask, val, offset) exchanges values between threads in a warp.
        // It uses the bitwise XOR of the thread’s lane index and 'offset' to determine the partner lane.
        // After this call, 'other' holds a value from another lane in the warp.
        float other = __shfl_xor_sync(mask, val, offset);
        // Compare the current thread’s val with the received 'other' and store the maximum.
        // After this step, 'val' is the max of the two compared values.
        val = fmaxf(val, other);
    }

    if (laneId == 0) {
        warp_max[warpId] = val;
    }

    // Reduce warp maxima (assuming bc=128 here; adjust if bc=256)
    if (threadIdx.x == 0) {
        float m = warp_max[0];
        m = fmaxf(m, warp_max[1]);
        m = fmaxf(m, warp_max[2]);
        m = fmaxf(m, warp_max[3]);
        // If bc=256, add more steps here for warp_max[4..7]
        warp_max[0] = m; 
    }

    __syncthreads();

    // Compute exp(val - row_max)
    float exp_val = __expf(sum - row_max);
    d_pij[i * bc + col] = exp_val;
    // To be continued...
```

**Illustration:**

```
Initial values:
 lanes:  0    1    2    3    ...  31
 vals:  v0,  v1,  v2,  v3,  ...  v31

After offset=16 step:
 We combine pairs (0,16), (1,17), (2,18), etc.
 Each pair now holds max(val_of_lane0, val_of_lane16), etc.

After offset=8 step:
 Each group of four threads (e.g., lanes 0,1,2,3 combined with lanes 8,9,10,11 previously) now shares the maximum among them.

This pattern continues until after offset=1, where the entire warp's maximum value is known in every thread.
```

Here I declared `warp_max[4]` for a total of `128` threads (each warp consists of `32` threads), corresponding to `128` in `bc`. The mask specifies which threads in the warp are active (participating in the shuffle operation). A full-warp mask (e.g., `0xffffffff` for a warp of `32` threads) is used to include all threads. After the calculation each thread maintains the same value, so only one thread should be responsible for writing the results.

Then `fmaxf()` performs a comparison between two `float` values. It may be optimized at the hardware or compiler level, offering better performance than a custom `if-else` block. In GPU code, built-in functions are often mapped to efficient hardware instructions.

`__expf()` calculates the exponential in a more efficient way in `float`.

#### (d) Row Sums and Update Intermediate Results:

After exponentiation, sum over each row to get $l_{ij}(r)$, the local normalization factor for that block of columns. The reduction method is the same as finding the max, using warp-level reduction.
   
**Code:**

```c=
// --- Warp-Level Reductions for row_sum ---
__shared__ float warp_sum[4];
// Find row_sum of exp_val via warp reduction
val = exp_val;
#pragma unroll
for (int offset = 16; offset > 0; offset >>= 1) {
    float other = __shfl_xor_sync(mask, val, offset);
    val += other;
}

if (laneId == 0) {
    warp_sum[warpId] = val;
}
__syncthreads();

if (threadIdx.x == 0) {
    float s = warp_sum[0] + warp_sum[1] + warp_sum[2] + warp_sum[3];
    warp_sum[0] = s;
}

__syncthreads();

float row_sum = warp_sum[0];

// Write out row_max and row_sum
if (col == 0) {
    d_mij[i] = row_max;
    d_lij[i] = row_sum;
}
```

Calculating `row_sum` with the method similar to `row_max`. Then write the results of `row_max` and `row_sum` into `mij` (stores the maximum) and `lij` (stores row sum), storing the values for the current tile. This will be used afterwards to update `mi` and `li` arrays for the entire input.

#### (e) Updating the Output `O`:

Wait until row_max, row_sum, and p_ij are computed. One thread per row is required to compute `mi_new_val` and `li_new_val`. The two values are used to update `mi[i]` and `li[i]` to reflect the new global maxima and sums after including the current tile. Finally, update the output `O_i`. `O_i` represents the weighted sum of the `V` values over all processed columns. `pv` is the contribution from the current tile: I take the computed probabilities `pij` for this tile and weight the `V` values accordingly. This gives the partial output contribution from the newly processed columns.

For the last part I combined the old output `oi_ij` (scaled to the new baseline) with the new partial result `pv` (also scaled to the new baseline). This ensures that `O_i` remains consistent and normalized with respect to the updated scaling factors.

### 1.2 Explain how matrices `Q`, `K`, and `V` are divided into blocks and processed in parallel.

#### (a) Choose Tile Dimensions (`br` and `bc`):

Select block (tile) sizes along the N dimension for both rows (br) and columns (bc). Here I select `br = 128` and `bc = 128` because they map well to GPU warps and threads, enabling efficient parallel reductions and memory access patterns. Why not larger? Because according to the input, the minimum `N` is `128`. Choosing larger tile size is feasible but requires more boundary checking if the input testcase has sizes less than `br` and `bc`, so I simply use the smallest value.

#### (b) Row Tiles for `Q`:

`Q` is split into vertical strips (tiles) of size `br x d`. Each tile `Q_i` covers `br` consecutive rows and all `d` embedding dimensions.

#### (c\) Column Tiles for `K` and `V`:

`K` and `V` are split into horizontal strips (tiles) of size `bc x d`. Each tile `K_j` and `V_j` covers `bc` consecutive rows (when viewed transposed, these correspond to `bc` columns in the `S` matrix) and all `d` dimensions.

#### (d) Iterating Over Tiles:

Cover the entire `N x N` attention matrix `S` by looping over tile pairs `(i, j)`.
For each `i` (indexing the row tiles of `Q`), we select a tile `Q_i` of shape `br x d`.
For each `j` (indexing the column tiles of `K` and `V`), we select `K_j` and `V_j` of shape `bc x d`.
Together, `Q_i` and `K_j` produce a `br x bc` block of the attention scores $S_ij = Q_i * K_j^T / sqrt(d)$.

#### (e) Parallel Processing on the GPU:

The GPU uses a grid of thread blocks to handle multiple `(i, j)` tile pairs concurrently.
One block handles one row of the `br x bc` tile.
Inside the kernel, threads load the required portions of `Q_i` and `K_j`, compute the dot products, and store intermediate results.
Because tiles are independent of each other (except for final aggregation steps), many tiles can be computed in parallel, fully utilizing the GPU’s massive parallel capabilities.

### 1.3 Describe how you chose the block sizes B_r​ and B_c​ and why:

I chose `br = 128` and `bc = 128`. Here are my reasons: The smallest sequence length `(N)` in the given input specifications is `128`. Setting it larger than `128` (for instance, `256`) requires more handling of testcases smaller than `256`. Also, by setting it to `128` it is multiple of `32`, which is the number of threads within a warp, making it straightforward to apply warp-level primitives and parallel reductions.

### 1.4 Specify the configurations for CUDA kernel launches, such as the number of threads per block, shared memory allocation, and grid dimensions:

- **Block Dimensions:**

```c
dim3 blockFused(bc, 1);
```

Each block has `bc` threads, with each thread dealing with a column in `br x bc` tile.

- **Grid Dimensions:**

```c
dim3 gridFused(1, br);
```

A total of `br` blocks are launched, with each block dealing with a row in `br x bc` tile.

- **Shared Memory Allocation:**

```c
// --- Warp-Level Reductions for row_max and row_sum ---
__shared__ float warp_max[4]; 
__shared__ float warp_sum[4];
```

I only allocated a total of `8` float elements for a block, storing the local maximum and local sum.

### 1.5 Justify your choices and how they relate to the blocking factors and the SRAM size.

The chosen block width `(bc = 128)` is a multiple of the warp size `(32)`. This simplifies the implementation of warp-level operations (like maximum and sum reductions) since `128` elements can be split evenly across four warps of `32` threads each. By doing so, operations like `__shfl_xor_sync()` for reductions are applied cleanly, without dealing with "leftover" threads or irregular boundaries. This ensures that every reduction step is efficient and straightforward, reducing both code complexity and runtime overhead.

As for the shared memory, I only used two arrays with four elements each to store the intermediate maximum and sum.

---

## 2. Profiling Results(Testcase: t08)

|          Metric Name          |         Metric Description          |     Min     |     Max     |     Avg     |
|:----------------------------:|:------------------------------------:|:----------:|:----------:|:----------:|
|   achieved_occupancy        |         Achieved Occupancy           |  0.967336  |  0.980284  |  0.973838  |
|      sm_efficiency          |     Multiprocessor Activity          |   89.97%   |   94.12%   |   92.77%   |
|     gld_throughput          |   Global Load Throughput (GB/s)      |  767.46    |  782.91    |  774.57    |
|     gst_throughput          |  Global Store Throughput (GB/s)      |  3.4648    |  3.5346    |  3.4969    |
| shared_load_throughput      | Shared Memory Load Throughput (GB/s) |  3.7798    |  3.8559    |  3.8148    |
| shared_store_throughput     |Shared Memory Store Throughput (GB/s) |  3.7798    |  3.8559    |  3.8148    |

- **Achieved Occupancy (~0.97–0.98):** Very close to maximum possible occupancy, indicating many active warps per SM.
- **SM Efficiency (~90–94%):** High multiprocessor utilization; SMs are rarely idle.
- **Global Load Throughput (~775 GB/s):** Heavy, well-coalesced reads from global memory.
- **Global Store Throughput (~3.50 GB/s):** Relatively small amount of data written back.
- **Shared Memory Throughput (~3.8 GB/s):** Shared memory is rarely utilized in my code.

---

## 3. Experiment & Analysis

### 3.1 System Specifications
- Apollo GPU Cluster

### 3.2 Optimization Techniques

- **Shared Memory:** I utilized shared memory to load data from `Q` and `K`. Since the transfer speed of shared memory is a lot faster than global memory, I expect it to run faster than the version not using shared memory. However, due to the limitation of the amount of shared memory on GTX1080(48KB only), for large testcases(Ex. `br = 128`, `bc = 128`), the shared memory will not fit(unless optimize the algorithm to use shared memory in a more efficient way). Using shared memory for smaller blocks usually result in faster execution. Using global memory with larger tiles runs faster on testcases with large `Q` and `K`.
- **Warp-Level Reduction:** Warp-level reductions operate on data that is local to the threads of a single warp. Because warp threads run in a tightly coupled manner and share a fast, hardware-supported communication path, these reductions can be implemented without costly global or even shared memory accesses for intermediate steps.
   Here I used `__shfl_xor_sync()` instruction to find row sum and row max efficiently.
- **Kernel Fusion:** I fused every operation into a single kernel. Fusing these operations into a single kernel cuts down on the number of launches, reducing overhead and improving overall throughput.
-  **Memory Coalescing:** The indexing schemes for `Q`, `K`, `V`, and `O` are chosen so that consecutive threads access consecutive elements in memory. This leads to more coalesced global memory accesses, which reduce memory transaction overhead and improve effective bandwidth utilization.
-  **Pre-Computations on Host:** Certain constants (like the scaling factor scalar = 1.0f / sqrtf(d)) are computed on the host and passed into the kernel. This avoids recalculating them repeatedly inside the kernel and saves some GPU instructions.
-  **Efficient Thread and Block Sizing:** I chose `br = N`, `bc = 128` for my final optimized code. I didn't utilize shared memory for the final version, so I get to set the tile size whichever I want. Setting `br = N` the entire row dimension is processed by a single block in the row direction. This eliminates the need to split the row dimension into multiple tiles and handle edge cases. Setting `bc = 128` ensures the block width is a multiple of `32`. Also, this is the smallest input block size, so no need to handle blocks smaller than `128`.
- **Minimize cudaMemCpy():** In the sequential version(also the GPU baseline), lots of memory copy is used to update the arrays. However, this slows the execution since memory copy is time exhausting. Instead, I use pointers to directly point to the memory location where the new values should be updated to avoid excessive memory operations.

### 3.3 Comparisons W/O Shared Memory

- **Small Block size(t11)**
    - (B, N, d): (13600, 128, 32)
    - Using `br = 32`, `bc = 64`
    - Why not use larger tile size? -> Ensure all testcases can fit into memory

- **d = 32:**

|   br   |   bc   |   d    | Shared Mem (floats) | Shared Mem (bytes) | Shared Mem (KB) |
|:------:|:------:|:------:|:-------------------:|:------------------:|:---------------:|
|   32   |   32   |   32   |        3,072        |       12,288       |      12 KB      |
| **32** | **64** | **32** |      **5,120**      |     **20,480**     |    **20 KB**    |
|   32   |  128   |   32   |        9,216        |       36,864       |      36 KB      |
|   64   |   32   |   32   |        4,096        |       16,384       |      16 KB      |
|   64   |   64   |   32   |        6,144        |       24,576       |      24 KB      |
|   64   |  128   |   32   |       10,240        |       40,960       |      40 KB      |
|  128   |   32   |   32   |        6,144        |       24,576       |      24 KB      |
|  128   |   64   |   32   |        8,192        |       32,768       |      32 KB      |
|  128   |  128   |   32   |       12,288        |       49,152       |      48 KB      |

- **d = 64:**

| br   | bc   | d   | Shared Mem (floats) | Shared Mem (bytes) | Shared Mem (KB) |
|:------:|:------:|:-----:|:---------------------:|:---------------------:|:-----------------:|
| 32   | 32   | 64  | 6,144               | 24,576             | 24 KB           |
| **32**   | **64**   | **64**  | **10,240**              | **40,960**             | **40 KB**           |
| 32   | 128  | 64  | 18,432              | 73,728             | 72 KB           |
| 64   | 32   | 64  | 8,192               | 32,768             | 32 KB           |
| 64   | 64   | 64  | 12,288              | 49,152             | 48 KB           |
| 64   | 128  | 64  | 20,480              | 81,920             | 80 KB           |
| 128  | 32   | 64  | 12,288              | 49,152             | 48 KB           |
| 128  | 64   | 64  | 16,384              | 65,536             | 64 KB           |
| 128  | 128  | 64  | 24,576              | 98,304             | 96 KB           |

- **Comparison:**

| Shared Mem | Without Shared Mem |
| :--------: | :--------: |
| 4.001 secs     | 4.023 secs     |

- **Large Block Size(t29)**
    - (B, N, d): (4, 32768, 32)

| Shared Mem | Without Shared Mem |
| :--------: | :--------: |
| 18.793 secs     | 20.125 secs     |

- **For Large Tiles:**
    - (B, N, d): (4, 32768, 32)
    - `br = N`, `bc = 128` without shared memory, `br = 32`, `bc = 64` for shared memory

| Shared Mem | Without Shared Mem |
| :--------: | :--------: |
| 18.793 secs     | 5.796 secs     |

- **Conclusion:** In my implementation, shared memory version is limited to the amount of shared memory, so the tile size cannot be set to large, resulting in more iterations. Implementation without shared memory, however, isn't bounded to memory limit but rather to testcases. Here I set `bc = 128` is because the smallest `N` is `128`, and `br = N` allows the code to go through the entire rows within the tile in one iteration. This tradeoff seems efficient in my code, however, more optimizations might be made to utilize the shared memory even more, accelerating the execution.

- **Impact of Different `br` Sizes(`bc` Remains `128`):**

![execution_time_vs_br](https://hackmd.io/_uploads/HkSNcZESJg.png)

![speedup_vs_br](https://hackmd.io/_uploads/SyHHcbNBye.png)

When tile size `(br)` is small, each tile handles fewer elements, so the kernel must launch more frequently and load data from global memory more often. This overhead leads to longer total execution times. As `br` increases, each launch processes more data per tile, improving data reuse in registers or shared memory and reducing kernel launch overhead—hence execution time drops. Once `br` grows large enough, performance starts to plateau because additional increases do not bring as much benefit, and may even cause resource contention (shared memory usage, limited occupancy, etc.), slightly increasing or stabilizing execution times.

---

## 4. Experience & Conclusion

I've tried different methods to optimize my code. At first, the use of shared memory is attempted, since the speed is much, much faster than global memory. However, due to the limit of shared memory size(usually 48 KB on Nvidia GPUs), tile size is limited. So I decided to try with larger tile size without shared memory, and it resulted in much faster execution time. There might be other methods to optimize the use of shared memory(etc. let each thread deal with more data instead of only one), but I decided to stick with the method without shared memory. Kernel fusion is also used to prevent multiple kernel launches. I also utilized warp reduction to accelerate the process of calculating the row max and row sum.

I also eliminated the use of `cudaMemCpy(,, cudaMemcpyDeviceToDevice)` to copy the updated results. Instead, I let the pointers point to the memory location and store the new data directly. This allowed my program to speed up even more with less memory copy operations.

It's an interesting lab, and I learned to utilize different methods to try and optimize my code.

---
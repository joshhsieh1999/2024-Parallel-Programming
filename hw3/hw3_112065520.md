# PP HW 3 Report

## 1. Implementation

### 1.1 Which algorithm do you choose in hw3-1?

I use the same code, but did some optimizations. First, I avoid recalculating the same data over and over in the same `for` loop.

```c=
int block_internal_start_x = b_i * B;
int block_internal_end_x = (b_i + 1) * B;
int block_internal_start_y = b_j * B;
int block_internal_end_y = (b_j + 1) * B;
```

I moved this to the second loop that iterates the entire tile. This way, when the innermost `for` loop is operating on the phases it won't be recalculated every time.

I also used `#pragma omp parallel` to let multiple threads execute the inner loop because each iteration is independent. `#pragma omp simd` is also used to accelerate the calculation.

I didn't spend too much time on optimizing the code since I passed all the testcases with those optimizations.

### 1.2 How do you divide your data in hw3-2, hw3-3?
- **Explain your data partitioning strategy** for both single-GPU (hw3-2) and multi-GPU (hw3-3).

**hw3-2: (Single-GPU Version)**

Here I try to utilize as much shared memory as possible.
```css
     64 columns
 0  ------------------------- 31 --- 32 ------------------------ 63
    |<------ 32 cols ----->|        |<------ 32 cols ------->|
    |         SUB-TILE A   | SUB-TILE B                        |
    |  localStore[ly][lx]  | localStore[ly][lx + 32]           |
    | (top-left sub-tile)  | (top-right sub-tile)              |
31  +-----------------------+-----------------------------------+
32  |         SUB-TILE C   | SUB-TILE D                        |
    | localStore[ly+32][lx]| localStore[ly+32][lx + 32]        |
    |(bottom-left sub-tile)|(bottom-right sub-tile)            |
63  +-----------------------+-----------------------------------+
            64 rows
```

**Explanation:**

Here I define each tile with size `64 x 64`. But on `GTX1080`, only `32 x 32` block size is supported. The reason why I define the tile size as `64 x 64` is because the shared memory size on GTX1080 is `48KB`.

A simple calculation of used shared memory for each kernel:

- **PHASE 1:**
    - Only one block is calculated(pivot block), so one `__shared__` array is used.

    ```c=
    #define BLOCK_SIZE 64
    __shared__ int pivotData[BLOCK_SIZE][BLOCK_SIZE];
    ```
    $$
64 \times 64 \times 4 \text{ bytes/int} = 16384 \text{ bytes} = 16 \text{ KB}
$$

- **PHASE 2:**
    - Pivot row and pivot column blocks need to be updated using the updated pivot block. I allocated three arrays of shared memory:
    
    ```c=
    __shared__ int pivotData[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ int rowData[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ int colData[BLOCK_SIZE][BLOCK_SIZE];
    ```
    $$
64 \times 64 \times 4 \text{ bytes/int} \times 3 = 49152 \text{ bytes} = 48 \text{ KB}
$$
    
    - Each block will utilize `rowTile` and `colTile` shared memory. Each block updates `2` tiles -> one pivot row tile and one pivot column tile. `pivotTile` is required to update the pivot row or pivot column block.
    
- **PHASE 3:**
    - All the other blocks should be updated. Each block is updated by one pivot row block and one pivot column block. The declaration is the same as above(using `64KB`).

    ```c=
    __shared__ int mainTile[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ int rowTile[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ int colTile[BLOCK_SIZE][BLOCK_SIZE];
    ```
    
- After finishing the calculation, write the result back to global memory and finish the program. 

**hw3-3:(Multi-GPU Version)**

The multi-GPU version still runs the three-phase logic like a single-GPU approach, both GPU update the same pivot block and pivot row/column in `phase 1` and `phase 2`. The difference then shows up in Phase 3, where the top half of the matrix is processed by GPU0 and the bottom half by GPU1 for the leftover blocks.

For example, for a total of `6` rounds:
`GPU0` -> row0 ~ row($\left\lfloor \frac{\text{rounds}}{2} \right\rfloor - 1$) -> row0 ~ row2
`GPU1` -> row($\left\lfloor \frac{\text{rounds}}{2} \right\rfloor$) ~ row(rounds - 1) -> row3 ~ row5

`round = 0:`
```lua
Tile blocks: (rowBlock, colBlock)
    
      c=0(pc)    c=1    c=2    c=3    c=4    c=5
   +------+------+------+------+------+------+
r=0(pr)|0,0(p)   0,1   0,2   0,3   0,4   0,5|
   +------+------+------+------+------+------+
r=1|1,0   1,1   1,2   1,3   1,4   1,5|
   +------+------+------+------+------+------+
r=2|2,0   2,1   2,2   2,3   2,4   2,5|
   +------+------+------+------+------+------+
r=3|3,0   3,1   3,2   3,3   3,4   3,5|
   +------+------+------+------+------+------+
r=4|4,0   4,1   4,2   4,3   4,4   4,5|
   +------+------+------+------+------+------+
r=5|5,0   5,1   5,2   5,3   5,4   5,5|
   +------+------+------+------+------+------+
```

`pivot block:(labeled as p)` tile(0,0) -> Both GPUs updates the pivot block
`pivot row & pivot column:` -> Both GPUs update the pivot row(r=0) and pivot column(c=0).
`remaining tiles:` -> `GPU0` updates row1 and row2(row0 is the pivot row and doesn't require updating), while `GPU1` updates row3 ~ row5. After `phase 3` onto the next iteration, `GPU1` fetches pivot row data from `GPU0` in order to update the value when it reaches `phase 3`.

### 1.3 What’s your configuration in hw3-2, hw3-3? And why? 

**Ans:**

- **hw3-2:**

    - **Blocking Factor:**
        - Each tile is `64 by 64`. This is used to utilize the entire shared memory.

    - **Grid Size:**
        - `Phase 1` uses 1 block only. The block is `32 by 32` in size, matching the limit of `1024` threads in 1 block on GTX1080.
        - `Phase 2` uses a grid of (`1, totalRounds`). Each block processes one row block and one column block at the same time. A total of `2 x (totalRounds - 1)` tiles need to be processed, and the 2 excessive blocks will exit.
        - `Phase 3` uses a grid of (`totalRounds, totalRounds`), every other tile block needs to be updated in the 2D grid except the pivot’s row/column. This may seem odd since only `(totalRounds - 1) x (totalRounds - 1)` blocks need to be calculated. But using `totalRounds x totalRounds` simplify the indexing in a kernel launch, just let the pivot row and pivot column blocks remain return immediately.

    - **Block Size:**
        - Each block is fixed, using `32 x 32` threads. This utilize the entire resource within one block on GTX1080.

- **hw3-3:**

Both blocking factor and grid size are basically the same as `hw3-2`. The only difference is `when` and `how` the tiles are updated.

### 1.4 How do you implement the communication in hw3-3?

```c=
for (int pivot = 0; pivot < rounds; pivot++) {
    // ...
}
```

Iterate over all the pivot blocks(`pivot`,`pivot`).

```cpp=
if ((tID == 0) && (pivot < p3Grid.y)) {
    cudaMemcpy(D + pivot * B * V, dArr[tID] + pivot * B * V,
               B * V * sizeof(int),
               cudaMemcpyDeviceToHost);
} else if ((tID == 1) && (pivot >= offsetY)) {
    cudaMemcpy(D + pivot * B * V, dArr[tID] + pivot * B * V,
               B * V * sizeof(int),
               cudaMemcpyDeviceToHost);
}

#pragma omp barrier

// if GPU0 doesn't own pivot, fetch from host
if ((tID == 0) && (pivot >= p3Grid.y)) {
    cudaMemcpy(
        dArr[tID] + pivot * B * V,
        D + pivot * B * V,
        (size_t)B * V * sizeof(int),
        cudaMemcpyHostToDevice
    );
}
// if GPU1 doesn't own pivot, fetch from host
else if ((tID == 1) && (pivot < offsetY)) {
    cudaMemcpy(
        dArr[tID] + pivot * B * V,
        D + pivot * B * V,
        (size_t)B * V * sizeof(int),
        cudaMemcpyHostToDevice
    );
}
```

In the first `if-else`, the GPU accountable for the pivot row copies the pivot row back to host memory. A barrier is used to ensure the data finishes copying before the other GPU fetches the data from host memory. After the other GPU fetches the data, both GPU begin to run the three phases.

### 1.5 Briefly describe your implementations (diagrams/figures/sentences)

- **hw3-2:**
    - **PHASE 1:** In this phase, only the pivot block needs to be updated. First, each thread loads `4` data into shared memory as shown below:
    ```scss
              <---  32 columns  ---> <---  32 columns  --->
  +-------------------------------+-------------------------------+
  |         TL (32×32)            |         TR (32×32)            |
  |                               |                               |
  |  pivotData[ly][lx]            |  pivotData[ly][lx+32]         |
  |                               |                               |
  |  <-- Each thread (lx,ly) -->  |  <-- same (lx,ly), but col+32 |
  |      loads exactly this       |      loads exactly this       |
  +-------------------------------+-------------------------------+
  |         BL (32×32)            |         BR (32×32)            |
  |                               |                               |
  |  pivotData[ly+32][lx]         |  pivotData[ly+32][lx+32]      |
  |                               |                               |
  |  <-- same (lx,ly), but        |  <-- same (lx,ly), but        |
  |       row+32 -->              |       row+32 AND col+32 -->   |
  +-------------------------------+-------------------------------+

    ```
    
    Each thread (ly,lx) is responsible for four specific elements in shared memory:
    `pivotData[ly][lx]`, `pivotData[ly+32][lx]`, `pivotData[ly][lx+32]`, `pivotData[ly+32][lx+32]`
    After loading the data, the thread performs the floyd-warshall operation on the four threads, iterating through the `64 x 64` tile. At the end of each iteration a `__syncthreads()` is used to ensure the next iteration only starts if all threads are updated. Write back to global memory after finishing the floyd-warshall operation.
    
    - **PHASE 2:** The concept is similar to phase 1. The difference is that each block in `phase 1` is only responsible for `1` sub-tile, while in `phase 2`, each block is responsible for `2` sub-tiles, which includes `1` pivot row sub-tile and `1` pivot column sub-tile.

    ```scss
              Column tile index ( j )
         0     1      2     ...    r-1     r      r+1   ...   N-1
         +------+------+------+-----+-------+-------+------+-----+
    i=0  |(0,0) |(0,1)*|(0,2)| ... |(0,r-1)|(0,r)  |(0,r+1)| ... |
         +------+------+------+-----+-------+-------+------+-----+
    i=1  |(1,0)*|(1,1)#|(1,2)| ... |(1,r-1)|(1,r)  |(1,r+1)| ... |
         +------+------+------+-----+-------+-------+------+-----+
    i=2  |(2,0) |(2,1) |(2,2)| ... |(2,r-1)|(2,r)  |(2,r+1)| ... |
         +------+------+------+-----+-------+-------+------+-----+
    ...  |      |      |      | ... |       |       |      | ... |
         +------+------+------+-----+-------+-------+------+-----+
    i=r-1|(r-1,0)|(r-1,1)|(r-1,2)|...|(r-1,r-1)|(r-1,r)|(r-1,r+1)|.. |
         +------+------+------+-----+-------+-------+------+-----+
    i=r  |(r,0) |(r,1) |(r,2) | ... |(r,r-1) |(r,r)  |(r,r+1) | ... |
         +------+------+------+-----+-------+-------+------+-----+
    i=r+1|(r+1,0)|(r+1,1)|(r+1,2)|.. |(r+1,r-1)|(r+1,r)|(r+1,r+1)|.. |
         +------+------+------+-----+-------+-------+------+-----+
    ...  |      |      |      | ... |       |       |      | ... |
         +------+------+------+-----+-------+-------+------+-----+
    i=N-1|(N-1,0)|(N-1,1)|(N-1,2)|.. |(N-1,r-1)|(N-1,r)|(N-1,r+1)|.. |
         +------+------+------+-----+-------+-------+------+-----+
    ```

    Each block updates a row tile (r,c)(in the pivot row) and a column tile (c,r)(in the pivot column), $c \neq r$. In the above diagram, suppose `(1,1)` is the pivot block(marked with `#`), then `(1,0)` and `(0,1)` are updated in the same block in `phase 2`(marked with `*`).
    So phase 2 loads the data of the two tiles into shared memory first, then perform floyd-warshall computation on the two tiles respectively. The indexing of the two tiles are written inside `floydPhase2()` function together, but they are actually independent calculations.

    - **PHASE 3:** For the last phase, each thread blocks loads three tiles into shared memory: `mainTile`, `rowTile`, and `colTile`. `mainTile` is the tile for updating, while `rowTile` and `colTile` represents the pivot row and pivot column. The update looks like this:

    $$
    \texttt{mainTile[i][j]} = \min\bigl(\texttt{mainTile[i][j]},\;\texttt{rowTile[i][k]} + \texttt{colTile[k][j]}\bigr).
    $$
    
    After each thread completed loading the data into shared memory and reached `__syncthreads()`, they perform the computation of `floyd-warshall` on the loaded data and update the values, and write it back to global memory.


- **hw3-3:**
    - Here the three phases are basically the same as single GPU. The only difference is each GPU handles a portion of the entire input.

    ```c=
    int offsetY = (tID == 0) ? 0 : (rounds / 2);

    // copy portion to device
    cudaMemcpy(
        dArr[tID] + offsetY * B * V,
        D + offsetY * B * V,
        (size_t)p3Grid.y * B * V * sizeof(int),
        cudaMemcpyHostToDevice
    );
    ```

    At the beginning, `GPU0` is loaded with the top half and `GPU1` is loaded with the bottom half. Then in the `for` loop running the floyd-warshall algorithm both GPUs copy their responsible data back to host. So take the first iteration as an example, `GPU0` bears the data of the top half, but `GPU1` requires the pivot row resided in `GPU0`. So it fetches the data from host memory. After both GPUs receive the pivot row they will begin the three phases, and do the same operations for the next iteration.

---

## 2. Profiling Results (hw3-2)

**Using NVIDIA profiling tools** (e.g., `nvprof`, `ncu`, or Nsight Compute), provide the following metrics for the biggest kernel of your hw3-2 program:

1. **Occupancy**
2. **SM efficiency**
3. **Shared memory load/store throughput**
4. **Global load/store throughput**

Here I profiled my `floydPhase3()` kernel with testcase `p11k1`, which is the largest kernel because it updates all other tiles in a testcase(size of `(totalRounds - 1) x (totalRounds - 1)`).

| Metric Name              | Metric Description                | Min         | Max         | Avg         |
|--------------------------|-----------------------------------|------------:|------------:|------------:|
| achieved_occupancy       | Achieved Occupancy               | 0.945071    | 0.947481    | 0.946694    |
| sm_efficiency           | Multiprocessor Activity           | 99.83%      | 99.89%      | 99.87%      |
| shared_load_throughput  | Shared Memory Load Throughput     | 3291.5GB/s  | 3344.8GB/s  | 3318.2GB/s  |
| shared_store_throughput | Shared Memory Store Throughput    | 267.33GB/s  | 271.66GB/s  | 269.50GB/s  |
| gld_throughput          | Global Load Throughput            | 200.50GB/s  | 203.75GB/s  | 202.12GB/s  |
| gst_throughput          | Global Store Throughput           | 66.833GB/s  | 67.915GB/s  | 67.374GB/s  |


- **Explanation:**
    - **achieved_occupancy:** An average of `94.67%` of warps are active on each streaming processor. This is a relatively high number, possibly because of the tiling strategy and block size of `32 x 32`.
    - **sm_efficiency:** Here the `sm_efficiency` is `99.87%` on average, meaning the GPU is almost running all the time(little idle time). Since I only load data into shared memory once at the beginning of the kernel, within the `for` loop that performs the floyd-warshall updates do not stall due to global loads, which contributes to a new efficiency.
    - **shared_load_throughput:** Since each distance check $$
    \texttt{mainTile[i][j]} = \min\bigl(\texttt{mainTile[i][j]},\;\texttt{rowTile[i][k]} + \texttt{colTile[k][j]}\bigr).
    $$ iterates `64` times per tile element, the reading performed on shared memory is large.
    - **shared_store_throughput:** Three elements should be read from shared memory to perform the updating process, then written back to an element. This is possibly the reason it has lower value than `shared_load_throughput`.
    - **gld_throughput vs gst_throughput:** At the beginning of the kernel, three tiles are required to be loaded to shared memory, while only one value is stored back to global memory at the end, possibly leading to a lower value in global store than in global load. 

For each metric:
- **Present the numeric results** or screenshots of the profiler output.
- **Explain** what the metrics indicate and how they relate to your program’s performance.

---

## 3. Experiment & Analysis

### 3.1 System Spec
- Apollo GPU Server
- Testcase: c20.1
- Profiled Phase 3 Kernel

### 3.2 Blocking Factor (hw3-2)

| BLOCK_SIZE | Execution Time (s) | GOPS  | Global Mem BW (GB/s) | Shared Mem BW (GB/s) |
|:----------:|:------------------:|:-----:|:--------------------:|:--------------------:|
| 16         |         0.422          |   1.93   |          366.2           |          1647.7           |
| 32         |         0.162          |   8.713   |          358.64           |          3093.36           |
| 64         |         0.120          |   21.76    |          266.37            |          3546.07           |

![gops_vs_blocksize_bar](https://hackmd.io/_uploads/ry8ZYxFBkl.png)

![shared_bw_vs_blocksize_bar](https://hackmd.io/_uploads/rkvGYltH1e.png)

![global_bw_vs_blocksize_bar](https://hackmd.io/_uploads/rJ6BYgtSkl.png)

**Observation:**
- **GOPS:** It increases significantly as the block size grows from 16 → 32 → 64. Larger blocks can process more data in parallel, reducing overhead and allowing more instructions to be executed per unit time. As the block size goes up, there’s often better reuse of data in shared memory, reducing idle cycles and boosting overall operations/second.
- **Global Memory Bandwidth:** With larger blocks, the kernel relies more on shared memory (and reuses data) rather than continuously streaming from global memory, so the measured global throughput actually drops. In contrast to shared memory, which rises as the block size increases.
- **Shared Memory Bandwidth:** With larger block size, for each phase the kernels operate on more data in shared memory, leading to higher bandwidth. 

### 3.3 Optimization (hw3-2)


| Version                                       | Block Size (B\) | Time (ms) |
|:--------------------------------------------- | ---------------:| ---------:|
| **CPU Blocked Floyd–Warshall**                |             512 |    300000 |
| **GPU Baseline FW (no optimization)**         |              32 |   3369.88 |
| **GPU Shared Memory(32 x 32)**                                              |               32  |         372.533  |
| **GPU Optimized FW(Final Optimized Version)** |              64 |   136.450 |

![times](https://hackmd.io/_uploads/HyrzuNKSkg.png)

![speedup](https://hackmd.io/_uploads/Bkyru4tHyg.png)

**Explanation:** 

For the GPU Baseline, no shared memory is used. Everything is written to and from the global memory. For the first optimized version, shared memory of size `32 x 32` is used. The selection for this size is because the largest thread block is of size `32 x 32`, which is `1024` threads. Each thread handles one element, loading them from global memory to shared memory. After loading them and each thread reaches `__syncthreads()`, the floyd-warshall operation begins and updates the elements. It can be seen on the graph that there's a huge enhancement using shared memory. I also ensured memory coalescing, that is, reading data from global memory sequentially to avoid a warp from reading to different memory blocks. After reading them to shared memory, it does not matter how the data were accessed. Loop unrolling is also implemented to accelerate the entire operation.

As for the final optimized version, this is done to fully utilize the shared memory limit on GTX1080, which is `48KB`. As mentioned above, each thread take cares of `4` data, so now the block size is increased to `64 x 64`. This also reduces the number of round iterations, leading to fewer kernel launches. With the improvements mentioned, the execution time lowered significantly and passed the testcases to get all the performance scores.

I also used `cudaHostRegister()` to page-locked(pinned) memory. This speeds up transfers from host to device in all optimized versions.

### 3.4 Weak Scalability (hw3-3)
- **Explain and present results** of your multi-GPU implementation as you scale the problem size:
  - How does runtime change as you increase the number of GPUs while keeping problem size per GPU constant?
  - Any unexpected behaviors or bottlenecks?

### 3.5 Time Distribution (hw3-2)
- **Break down** the time spent in:
  1. Computing (kernel execution)
  2. Communication (GPU-GPU or device-device)
  3. Memory copy (H2D, D2H)
  4. I/O (reading input files, writing outputs)

| **Input Size** | **I/O Input (ms)** | **Host→Device (ms)** | **Compute (ms)** | **Device→Host (ms)** | **I/O Output (ms)** | **Total (ms)** |
|----------------|--------------------:|----------------------:|-----------------:|----------------------:|---------------------:|---------------:|
| **p21k1**      | 752                | 327                  | 8294.3           | 134                  | 2604                | 12111.3        |
| **p22k1**      | 712                | 380                  | 9446.49          | 147                  | 2924                | 13609.49       |
| **p23k1**      | 903                | 404                  | 10827.9          | 161                  | 3210                | 15505.9        |
| **p24k1**      | 3610               | 400                  | 12267.1          | 181                  | 4272                | 20730.1        |
| **p25k1**      | 1970               | 448                  | 14288.1          | 196                  | 3801                | 20703.1        |

![timing_barchart](https://hackmd.io/_uploads/rkCIrIYHJl.png)

**Analysis:**

The compute time is the main contributor to the total runtime. As the input size increases, the compute portion within the entire runtime increases also. I/O times are also significant. Both input and output time grow noticeably with larger inputs. As for data transfers between host and device memory the communication time is relatively small compared to compute time and I/O time.

---

## 4. Experiment on AMD GPU

I ran both of them using `hipify-clang` on AMD GPU, but got TLE on some testcases. Possible because my code is written in cuda which is designed by Nvidia, and the compiler for cuda will optimize some operations within my code. But directly converting a cuda code to a hip code might not be as efficient as running the cuda code on amd GPU, resulting in slower execution times.

---

## 5. Experience & Conclusion

**What have you learned from this homework?**

The most important part is the usage of shared memory. Loads and stores from shared memory is much much quicker than global memory. Just focus on coalesced read and write from global memory to shared memory, and calculate the data within shared memory, minimize contact with global memory and the speed will be fast.

---
